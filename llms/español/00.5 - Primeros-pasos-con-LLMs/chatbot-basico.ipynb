{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estableciendo el entorno\n",
    "\n",
    "Primero que nada, debemos importar las librerías que utilizaremos que, en nuestro caso, serán el módulo `OpenAI` de la librería `openai` y la librería `os`.\n",
    "\n",
    "Recordemos que la API key que OpenAI debemos guardarla en un lugar seguro y nunca escribirla explícitamente en el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Cargamos nuestra API de OpenAI (en mi caso, la tengo guardada \n",
    "# como una variable de entorno para evitar copiarla aquí de forma explícita)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Inicializar el cliente de OpenAI\n",
    "llm = OpenAI(api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construyamos nuestro chatbot\n",
    "\n",
    "## Definiendo el rol de experto\n",
    "Luego, definimos el tipo de experto que será nuestro chatbot, estableciendo su función a través de un mensaje que luego le pasaremos al modelo de lenguaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = {\n",
    "'role': 'developer',\n",
    "'content': \"\"\"\n",
    "\tEres un asistente amigable y experto en Python. \n",
    "\tResponde las preguntas de manera clara y concisa.\n",
    "\t\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El corazón del chatbot\n",
    "\n",
    "Una de las partes más importantes es la función que se encarga de llamar el modelo de lenguaje en sí. Mediante esta función podremos pasarle a la API y, por lo tanto, al modelo, qué es lo que está pidiendo el usuario, cómo quiere recibir la respuesta, etc. Luego, llama al modelo de lenguaje para que procese su pregunta y devuelve el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_respuesta(dev_prompt, mensaje_usuario):\n",
    "    user_prompt = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": mensaje_usuario\n",
    "    }\n",
    "    # Llamada a la API    \n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            dev_prompt,\n",
    "            user_prompt\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Retorna el texto generado\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El programa principal\n",
    "\n",
    "Una vez que tenemos definida la función que realiza la llamada al LLM, creamos el programa principal, el cual contará con un bucle while para que luego de cada respuesta, el programa nos pida la siguiente pregunta, y así seguir preguntando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"¡Hola! Soy tu asistente en Python. Escribe 'salir' para terminar la conversación.\")\n",
    "\n",
    "while True:\n",
    "    mensaje_usuario = input(\"Tú: \")\n",
    "    if mensaje_usuario.lower() == \"salir\":\n",
    "        print(\"Asistente: ¡Hasta luego!\")\n",
    "        break\n",
    "\n",
    "    respuesta = obtener_respuesta(developer_prompt, mensaje_usuario)\n",
    "    print(f\"Asistente: {respuesta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primeros tests\n",
    "\n",
    "Al testear el programa nos damos cuenta que también nos responde preguntas no relacionadas con Python.\n",
    "Por eso, debemos modificar el `developer_prompt` para evitar que responda cosas que no están relacionadas con su función. Esto es lo que se le llama un *guardrail* o valla de seguridad (queda mejor en inglés)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = {\n",
    "    \"role\": \"developer\",\n",
    "    \"content\": \"\"\"\n",
    "        Eres un asistente amigable y experto en Python. \n",
    "        Responde las preguntas de manera clara y concisa.\n",
    "        Solo responde preguntas relacionadas con Python. \n",
    "        Comprueba si el pedido se ajusta a algún concepto relacionado con Python.\n",
    "        Si recibes una pregunta que no esté relacionada en absoluto con Python di que solo conoces sobre Python, nada más.\n",
    "    \"\"\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
